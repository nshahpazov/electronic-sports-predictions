---
title: "Dota"
author: "Nikola Shahpazov"
date: "12/3/2020"
output: html_document
---

```{r package-setup, message=FALSE, warning=FALSE, include=FALSE, results='hide'}
## Create the list of required packages
list.of.packages <- c(
  # utility libraries
  "Metrics",
  "reshape",
  "tidyverse",
  "caret",
  "recipes",
  "purrr"
)

new.packages <- list.of.packages[(!list.of.packages %in% installed.packages() |
                                    list.of.packages %in% old.packages()[ ,"Package"])]

if (length(new.packages)) {
  install.packages(new.packages, repos = "http://cran.us.r-project.org")
}

## Load all necessary packages in RAM (working environment)
lapply(list.of.packages, require, character.only = TRUE)

rm(list.of.packages, new.packages)
```

```{r constants}
WINDOW_COLS <- c("gold_difference", "xp_difference", "lh_difference")
```

```{r load-inner-modules}
source("helpers.R")
source("markov-chains-helpers.R")
```

### Loading data
```{r load-data}
matches_df <- read.csv("./data/match.csv",
                       colClasses = c(radiant_win = "logical"))
time_df <- read.csv('./data/player_time.csv')

matches_df$duration[1:20] / 60
```

### Preprocessing

```{r prepare-data}
diffs_by_time_df <- time_df %>%
  mutate(gold_difference = diff_means(., "gold_t_[0-4]$", "gold_t_1[2-3]")) %>%
  mutate(xp_difference = diff_means(., "xp_t_[0-4]$", "xp_t_1[2-3]")) %>%
  mutate(lh_difference = diff_means(., "lh_t_[0-4]$", "lh_t_1[2-3]")) %>%
  mutate(minute = times / 60) %>%
  select(match_id, minute, gold_difference, xp_difference, lh_difference) %>%
  pivot_wider(id_cols = "match_id", names_from = "minute",
              values_from = WINDOW_COLS) %>%
  select(-gold_difference_0, -xp_difference_0, -lh_difference_0) %>%
  left_join(select(matches_df, match_id, radiant_win, duration),
            by = "match_id") %>%
  mutate(radiant_win = as.factor(radiant_win)) %>%
  # drop outliers
  filter(duration / 60 <= 101)
```

### Train set and Test set split

```{r test-train-split}
train_indices <- createDataPartition(diffs_by_time_df$radiant_win,
                                     p = .8,
                                     list = FALSE,
                                     times = 1)

train_set <- diffs_by_time_df[train_indices, ]
test_set <- diffs_by_time_df[-train_indices, ]
```

### Train various logistic regression models

```{r train-time-windows-lr, include=FALSE}
tc <- trainControl(method = "cv", number = 3)

train_set_std <- standardize_time_df(train_set)
test_set_std <- standardize_time_df(test_set)

# slr_all <- train_lr(data = train_set_std, cols = WINDOW_COLS, control = tc)
# slr_gld <- train_lr(data = train_set_std, cols = WINDOW_COLS[1], control = tc)
slr_glh <- train_lr(data = train_set_std, cols = WINDOW_COLS[-2], control = tc)

lr_models <- list(
  # "slr_all" = slr_all,
  # "slr_gld" = slr_gld,
  "slr_glh" = slr_glh
)
```

### Explore Cross Validation Test Accuracy

```{r plot-cv-acuracy}
lr_models %>%
  map(~imap(., ~.$results$Accuracy)) %>%
  imap(~data.frame(accuracy = unlist(.), time = 6:101, model = .y)) %>%
  bind_rows() %>%
  ggplot(mapping = aes(x = time, y = accuracy, color = model)) +
  geom_line() +
  ggtitle("Training set CV Accuracy")
```

We see that SLR with gold and last hits (gold is correlated with xp) behaves the best till the 
80th minute and after that it starts oscillating a lot, probably because 
there aren't a lot of matches then, or the matches are very even.

```{r prediction-accuracies}
test_performance <- lr_models %>%
  imap(
    ~data.frame(
      accuracy = get_test_accuracies(., test_set_std, interval = 1:20),
      time = 1:20,
      model = .y
    )
  ) %>%
  bind_rows()

test_performance %>%
  ggplot(mapping = aes(x = time, y = accuracy, color = model)) +
  geom_line() +
  ggtitle("Test Set Accuracy")
```
We see that the test accuracy behaves similarly to the test set cv accuracy from above.

### Markov Chains Model

```{r markov-chain-model, message=FALSE, warning=FALSE}
# create winners transition matrix
NBIN <- 8
winners_tm <- train_set %>%
  # reflect() %>% 
  filter(as.logical(radiant_win)) %>%
  as_binned_matrix(nbin = NBIN) %>%
  construct_transition_matrix(nbin = NBIN)

# create losers transition matrix
losers_tm <-  train_set %>%
  # reflect(TRUE) %>% 
  filter(!as.logical(radiant_win)) %>%
  as_binned_matrix(nbin = NBIN) %>%
  construct_transition_matrix(nbin = NBIN)

# plot winners transition matrix
winners_tm %>%
  matrix_to_df() %>%
  ggplot(mapping = aes(row, col, fill = value)) + 
    geom_tile() +
    geom_text(aes(label = round(value, 1))) +
    ggtitle("Transition changes for the Winning team in the train set") +
    xlab("Gold Bin States") +
    ylab("Gold Bin States")

# plot losers transition matrix
losers_tm %>%
  matrix_to_df() %>%
  ggplot(mapping = aes(row, col, fill = value)) + 
    geom_tile() +
    geom_text(aes(label = round(value, 1))) +
    ggtitle("Transition changes for the Losing team in the train set") +
    xlab("Gold Bin States") +
    ylab("Gold Bin States")
```

```{r markov-chains-model-predictions, echo=FALSE}
# TODO: use the faster method of updating and accessing transition probabilities
# TODO: add prediction for the duration (discretized)
# TODO: add decision tree model to see the accuracy
# TODO: use the predicted probability as a predictor for the logistic regression
# TODO: make pca of gold and last hits
# TODO: add benchmark model with just the difference of gold \ last hits
# TODO: test markov chain model with different types of discretization

tsbm <- as_binned_matrix(test_set, nbin = NBIN)

mc_accuracies <- imap(1:80, ~mc_accuracy(tsbm, winners_tm, losers_tm, .))

bind_rows(
  test_performance[1:80, ],
  data.frame(
    accuracy = unlist(imap(1:80, ~acc(tsbm, winners_tm, losers_tm, .))),
    time = 1:80,
    model = "MC"
  )
) %>% 
ggplot(mapping = aes(x = time, y = accuracy, color = model)) +
geom_line() +
ggtitle("Test Set Accuracy")

# rows <- c(1, 2, 3, 4, 1)
# cols <- c(4, 2, 5, 1, 4)
# freq <- matrix(0, 5, 5)
# 
# out <- aggregate(val ~ rows + cols, cbind(rows, cols, val = 1), sum)
# freq[as.matrix(out[1:2])] <- out$val
# 
# freq[cbind(indices[,1], indices[,2])] <- freq[cbind(m[,1], m[,2])] + 1
```
```
We tried segmenting the markov chain models into different categories. When predicting
for a minute we use a model for which is trained on games longer than that minute
This didn't improve anything, probably because the gold transition changing is more
important in early stages of the game.

Tried different quantile bins for the gold states and seems that 8 gives better results than 10
