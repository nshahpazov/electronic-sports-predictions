---
title: "prior-classification"
author: "Nikola Shahpazov"
date: "1/16/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r package-setup, message=FALSE, warning=FALSE, include=FALSE, results='hide'}
library("reticulate")

list.of.packages <- c(
  "Metrics",
  "reshape",
  "glue",
  "tidyverse",
  "caret",
  "dbplyr",
  "nodbi",
  "mongolite",
  "recipes",
  "purrr",
  "jsonlite",
  "reticulate"
)

new.packages <- list.of.packages[(
  !list.of.packages %in% installed.packages() |
   list.of.packages %in% old.packages()[ ,"Package"]
)]

if (length(new.packages)) {
  install.packages(new.packages, repos = "http://cran.us.r-project.org")
}

## Load all necessary packages in RAM (working environment)
lapply(list.of.packages, require, character.only = TRUE)

rm(list.of.packages, new.packages)
```

```{r load-helper-files}
source("./helpers.R")
source("./prior-helpers.R")
```

### Loading the data
We are using 4 main data sets
* players - containing information about each match and each player in the 
match, what hero did hey used, how much gold and kills they produced. We use
this data frame for producing the played heroes data frame, i.e. a data frame 
where each row is a 224-dimensional vector for which the ith coordinate
corresponds whether the ith hero is in the radiant team and i + 112 is whether
the ith hero is in the dire team
* matches - this is the data frame which holds the target variable, i.e. 
radiant_win which we are trying to predict both for real time predictions and 
prior predictions. We join almost every other table by match_id with this one.
* hero_stats.json contains information about the different heroes, e.g.
attack range, vitality, attack range, etc.
* player_ratings contains information about the TrueSkill rating of each of the 
players involved. This one should probably be an important predictor for 
classification.

```{r loading-data}
players_df <- read.csv("./data/players.csv")
player_ratings_df <- read.csv("./data/player_ratings.csv")
matches_df <- read.csv(
  "./data/match.csv",
  colClasses = c(radiant_win = "logical")
)

hero_stats_df <- "./data/hero_stats.json" %>%
  jsonlite::fromJSON(flatten = TRUE) %>%
  select(
    hero_id, attack_range, projectile_speed,
    attack_rate, move_speed, turn_rate,
    starts_with("base"), ends_with("gain")
  )
```
## Feature Engineering

### Hero selection feature engineering

We produce the selected heroes data frame containing a a 224 dimensional vector 
where x_n = 1 if the nth hero is in the radiant team and x_(n+113) if the nth 
hero is in the dire team for the match

```{r feature-engineering-hero-selection}
heroes_df <- players_df %>%
  mutate(hero_id = ifelse(player_slot > 5, hero_id + 113, hero_id)) %>%
  mutate(val = 1) %>%
  pivot_wider(
    names_from = "hero_id",
    values_from = "val",
    values_fill = 0,
    names_glue = "hero_{hero_id}"
  ) %>%
  select(match_id, starts_with("hero")) %>%
  select(-hero_damage, -hero_healing) %>%
  group_by(match_id) %>%
  summarise(across(everything(), ~sum(.x, na.rm = TRUE)))
```
### Hero Statistics feature engineering

We create a dataframe containing the hero statistics of each of the ten players
for each game like mana generation, base health, strength, durability, etc.
For example attack_range_3 is the attack range of the fourth player

```{r feature-engineering-hero-stats}
matches_hero_stats_df <- players_df %>%
  select(match_id, hero_id, player_slot) %>%
  left_join(hero_stats_df, by = "hero_id") %>%
  select(-hero_id) %>%
  pivot_wider(
    names_from = "player_slot",
    values_from = colnames(hero_stats_df)[-1]
  ) %>%
  # impute missing values with 0 since they have 0 of that skill\statistic
  mutate(across(everything(), ~replace_na(.x, 0)))
```

### Winning rate of hero against other hero feature engineering

We create a data frame with all the combinations of different heroes and their
corresponding ratio of winning over losing calculated from all the games.
Using this dataset of only 50k matches in total, this feature engineering 
process does not produce useful information in the described manner. 
Having so little matches, producing a ratio of winning-losing for player 
playing a particular hero, we get NaNs almost for every account id.

```{r feature-engineering-heroes-winning-ratios}
# TODO: this should be done only on a training set to prevent data leakage
# TODO: do a join on the whole data frame
# TODO: join with player ranks as well
# TODO: remove rows with account id equal to 0
team_heroes_df <- generate_team_heroes_df(players_df)
team_heroes_ratios_df <- generate_ratios_df(team_heroes_df)

team_heroes_df %>%
  left_join(
    team_heroes_ratios_df,
    by = c("hero_id_team_1", "hero_id_team_2", "account_id")
  ) %>%
  arrange(match_id) %>%
  mutate(n = c(rep(seq(1, 5), 50000))) %>%
  mutate(ratio = ifelse(account_id == 0, 1, ratio)) %>%
   pivot_wider(
    id_cols = match_id,
    names_from = n,
    values_from = ratio,
    names_glue = "hero_combo_{n}"
  )
```
```{r feature-engineering-heroes-winning-ratios}
# we are not using those for now
hero_combos <- team_heroes_df %>%
  group_by(match_id) %>%
  expand(hero_id_team_1, hero_id_team_2) %>%
  ungroup()

hero_combos_ratios_df <- hero_combos %>%
  arrange(match_id) %>%
  mutate(n = c(rep(seq(1, 25), 50000 - 1))) %>%
  merge(team_heroes_ratios_df, by = c("team_1", "team_2")) %>%
  pivot_wider(
    id_cols = match_id,
    names_from = n,
    values_from = ratio,
    names_glue = "hero_combo_{n}"
  )

```
### Player rankings feature engineering
Using the table having all the rankings of the different players we produce 
again a data frame with for example trueskill_mu_{j} being the mean true skill 
for the jth player, again 0-5 being the amazing radiant team, and the 6-10 being
the not so amazing dire team.

```{r feature-engineering-player-rankings}
matches_player_rankings_df <- players_df %>%
  select(match_id, account_id) %>%
  left_join(player_ratings_df, by = "account_id") %>%
  arrange(match_id) %>%
  mutate(n = rep(seq(1, 10), 50000)) %>%
  pivot_wider(
    names_from = n,
    id_cols = match_id,
    values_from = c(trueskill_mu, trueskill_sigma)
  )
```
### Data Preparation
There are a lot of NaN values, some crazy big ones as well, 
maybe used as a placeholder for something. We need to impute so that there 
aren't any NaN or or wrong values  in order to fit a Logistic Regression or 
whatever model.

We clean the ratings, for some of the players we don't have
calculated TrueSkill rating. We don't have more than 5 percent missing values 
in that table.

```{r data-preparation}
sapply(matches_player_rankings_df, function(x) sum(is.na(x))) %>%
  as.data.frame()

# replace NaN values in the rankings with the mean of the particular column
matches_player_rankings_df <- matches_player_rankings_df %>%
  mutate_all(funs(ifelse(is.na(.), mean(., na.rm = TRUE), .)))
```
### Predictor transformations
Since we have the same variables for all of the players, it might be better to
take the mean of the variables in the first team and subtract from it the mean
of the same variables in the second team.

```{r predictor-transformations}
# overwrite it selecting the aggregated ratings
matches_player_rankings_df <- matches_player_rankings_df %>%
  mutate(
    trueskill_mu = diff_means(.,
      "trueskill_mu_[1-5]",
      "trueskill_mu_[6-9]|trueskill_mu_10"
    ),
    trueskill_sigma = diff_means(.,
      "trueskill_sigma_[1-5]",
      "trueskill_sigma_[6-9]|trueskill_mu_10"
    )
  ) %>%
  select(match_id, trueskill_mu, trueskill_sigma)

# same procedure for the hero statistics
stat_cols <- colnames(select(hero_stats_df, -hero_id))

for (i in 1:length(stat_cols)) {
  matches_hero_stats_df[stat_cols[i]] <- matches_hero_stats_df %>%
    diff_means(glue("{stat_cols[i]}_[0-4]$"), glue("{stat_cols[i]}_1[2-3]"))
}

# overwrite it selecting the only the aggregated columns 
matches_hero_stats_df <- select(matches_hero_stats_df, match_id, stat_cols)
```
### Train\test set data splitting
We join all the dataframes we have feature engineered so far into a single 
dataframe containing all. Then we split the data into training and test sets 
in a 80% \ 20% manner.

```{r}
train_indices <- createDataPartition(heroes_df$match_id, p = .8, list = FALSE)

match_data <- matches_df %>%
  select(match_id, radiant_win) %>%
  # join with the selected heroes vectors
  left_join(heroes_df, by = "match_id") %>%
  # join with true skill rankings
  # left_join(matches_player_rankings_df, by = "match_id") %>%
  # # join with the hero statistics
  # left_join(matches_hero_stats_df, by = "match_id") %>%
  mutate(radiant_win = as.factor(ifelse(radiant_win, "yes", "no")))

train_df <- match_data[train_indices, ]
test_df <- match_data[-train_indices, ]

# sanity check for nan values
match_data %>%
  map(~sum(is.na(.x))) %>%
  as.data.frame()
# all are 0s
```

### Feature Selection mechanisms for a Simple Logistic Regression
First, we use a simple approach of 5-times repeated 10-fold cross-validation 
where for each resample, we fit the two models on 90% of the resample, 
predict on the left 10% of the resample. We compute the ROC scores and do a 
one-sided t-test comparision for effect of introducing one variable to the 
null model containing only intercept. This approach is introduced more in 
[2]Kuhn et Johnson.

```{r exploring-one-predictor-effect-on-lr}
# TODO: move this function to a helper file for the feature engineering process
# we are not executing those if the csv is already generated (takes too much time)
compare_models <- function (model1, model2, metric = model1$metric[1], ...) {
  glm_resamples <- list(model1, model2) %>%
    resamples()

  model_difference <- glm_resamples %>%
    diff(metric = metric[1], ...)
    # we return all the statistics from the caret package
  model_difference$statistics[[1]][[1]]
}

null_matrix <- data.frame(intercept = rep(1, nrow(train_df)))

ctrl <- trainControl(
  method = "cv",
  classProbs = TRUE, 
  summaryFunction = twoClassSummary
)

set.seed(63331)

# baseline model 
# control the training using repeated cross validation
null_model <- train(
  x = null_matrix,
  y = train_df$radiant_win,
  preProc = "YeoJohnson",
  method = "glm",
  metric = "ROC",
  trControl = ctrl
)

# we initialize the results data frame as an empty one
features_to_try <- colnames(select(train_df, -match_id, -radiant_win))

one_predictor_results <- data.frame(
  Predictor = features_to_try,
  Improvement = NA,
  Pvalue = NA,
  Accuracy = NA,
  stringsAsFactors = FALSE
)

for (i in 1:nrow(one_predictor_results)) {
  set.seed(63331)
  current_columns <- c("radiant_win", one_predictor_results$Predictor[i])
  var_mod <- train(
    radiant_win ~ .,
    data = train_df[, current_columns],
    method = "glm",
    metric = "ROC",
    trControl = ctrl
  )

  tmp_diff <- compare_models(var_mod, null_model, alternative = "greater")

  metrics <- getTrainPerf(var_mod)[1, ]
  one_predictor_results$Accuracy[i] <- (metrics$TrainSpec + metrics$TrainSens) / 2
  one_predictor_results$ROC[i] <- metrics$TrainROC
  one_predictor_results$Improvement[i] <- tmp_diff$estimate
  one_predictor_results$Pvalue[i] <- tmp_diff$p.value
}

one_predictor_results %>%
  select(-Improvement) %>%
  arrange(desc(Accuracy))

write.csv(one_predictor_results, "./results/one-predictor-lr-effect.csv")
```


```{r}
one_predictor_results <- read.csv("./results/one-predictor-lr-effect.csv")

ctrl <- trainControl(
  method = "cv",
  classProbs = TRUE, 
  summaryFunction = twoClassSummary
)

best_predictors <- one_predictor_results %>%
  arrange(desc(Accuracy, Pvalue)) %>%
  # head(19) %>%
  # just take all
  .$Predictor

var_mod <- train(
  radiant_win ~ .,
  data = train_df,
  method = "glm",
  metric = "Accuracy",
  trControl = ctrl
)

rfe_ctrl <- rfeControl(
  functions = lrFuncs,
  method = "repeatedcv",
  repeats = 5,
  verbose = FALSE
)

# lmProfile <- rfe(
# train_df[, best_predictors], train_df$radiant_win,
#  sizes =  c(1:5, 10, 15, 20, 21),
#  rfeControl = rfe_ctrl
# )

train_df$radiant_win %>% table() %>% prop.table()
(0.480975 * var_mod$results$Spec + 0.519025 * var_mod$results$Sens)
```

### Train a model with 5 best features

```{r}

```


### Some simple modeling with regularization

We explore different classification mechanisms on the around 500-dimensional
data frame of different matches constructed by us from the previous feature 
engineering steps above. 

```{r training-models}
train_control <- trainControl(
  method = "cv",
  number = 5,
  summaryFunction = twoClassSummary,
  classProbs = TRUE
)

hero_glm_model <- train(
  form = radiant_win ~ .,
  data = heroes_train_df,
  trControl = train_control,
  method = "glmnet",
  metric = "Accuracy",
  family = "binomial"
)

hero_glm_model$results
```